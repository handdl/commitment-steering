{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15e81b48",
   "metadata": {},
   "source": [
    "**TLDR;** Our goal is to stimulate and find some \"feeling\" inside the LLM. Later by injecting with some coefficient alpha during the inference we can steer model's output. \n",
    "\n",
    "Our pipeline:\n",
    "1) We make snapshots of its activations while processing two datasets that differ only in something related to this \"feeling\" (building them is the most creative and interesting part). Here we hope that the statistical differences of activations will catch it.\n",
    "2) Then we search the right layer where direction of activation differences really controls something. Here we need to do some analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7054f9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate\n",
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4ee70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c3203c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\",)\n",
    "model.config.pad_token_id = model.config.eos_token_id # for warnings\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d807f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make snapshots we do something like (for usual and mimicry datasets)\n",
    "\n",
    "# from steering import make_hook\n",
    "# from dataset import generate_dataset, generate_mimicry_dataset\n",
    "#\n",
    "# dataset = generate_dataset(n=200, ask_truth=False, seed=42, mimicry=False)\n",
    "# activations = {i: [] for i in range(len(model.model.layers))}\n",
    "# handles = []\n",
    "#\n",
    "# for i, layer in enumerate(model.model.layers):\n",
    "#     handles.append(layer.register_forward_hook(make_hook(activations, i)))\n",
    "# for ex in dataset:\n",
    "#     prompt = build_prompt(ex[\"question\"], ex[\"thought\"])\n",
    "#     inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "#     with torch.no_grad():\n",
    "#         model(**inputs, use_cache=False)\n",
    "# for h in handles:\n",
    "#     h.remove()\n",
    "#\n",
    "\n",
    "# Then we make some analysis to find the right LAYER and measure interesting direction\n",
    "# LAYER = ... # some analysis\n",
    "# D = torch.stack([activations[LAYER][i] - activations_mimicry[LAYER][i] for i in range(len(activations[LAYER]))]).mean(dim=0)\n",
    "\n",
    "# Since I've already done it (for my simple synthetical datasets on arithmetics, see datasets.py) we can just load them:\n",
    "LAYER = 18\n",
    "D = torch.load(\"D.pt\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749e577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_response_with_injection(question, layer, d, alpha):\n",
    "    h = model.model.layers[layer].register_forward_hook(make_delta_hook(d, alpha))\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, max_new_tokens=1024, eos_token_id=tokenizer.eos_token_id)\n",
    "    print(f\"Under injection with alpha={alpha} response is:\\n{tokenizer.decode(out[0], skip_special_tokens=True)}\")\n",
    "    h.remove()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65773ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from steering import make_delta_hook\n",
    "\n",
    "print_response_with_injection(\n",
    "    question=\"Is Paris the capital of Germany? Give me quick answer,\\\n",
    "    then think about it and made a conclusion.\", \n",
    "    layer=LAYER, \n",
    "    d=D, \n",
    "    alpha=2.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a399ac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you notice some strange behaviour - use it to clean the model from all hooks\n",
    "def force_hooks_removal():\n",
    "    for module in model.modules():\n",
    "        module._forward_hooks.clear()\n",
    "        module._forward_pre_hooks.clear()\n",
    "        module._backward_hooks.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "custom_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
